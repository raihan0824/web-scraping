{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver    # to  control the chrome browser\n",
    "import time\n",
    "from bs4 import BeautifulSoup     # to parse the page source\n",
    "import pandas as pd                # to create csv file of scraped user details\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google lummoshop.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups1 = []\n",
    "url = 'https://www.google.com/search?q=site%3Alummoshop.com'\n",
    "options = Options()\n",
    "# options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups1.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        next_links = driver.find_elements(by=By.LINK_TEXT, value='Berikutnya')\n",
    "        next_links[0].click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups1.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 247\n"
     ]
    }
   ],
   "source": [
    "links1=[]\n",
    "for soup in soups1:\n",
    "    links1.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://lummoshop.com/'))])\n",
    "links1=sum(links1,[])\n",
    "links1=[re.findall('https://lummoshop.com/[a-zA-Z0-9]+',link)[0] for link in links1]\n",
    "print(f'Total URL found: {len(list(set(links1)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google tokko.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups2 = []\n",
    "url = 'https://www.google.com/search?q=site%3Atokko.io'\n",
    "# options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups2.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        next_links = driver.find_elements(by=By.LINK_TEXT, value='Berikutnya')\n",
    "        next_links[0].click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups2.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 221\n"
     ]
    }
   ],
   "source": [
    "links2=[]\n",
    "for soup in soups:\n",
    "    links2.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://tokko.io/'))])\n",
    "links2=sum(links2,[])\n",
    "links2=[re.findall('https://tokko.io/[a-zA-Z0-9]+',link)[0] for link in links2]\n",
    "print(f'Total URL found: {len(list(set(links2)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links2=[i.replace('tokko.io','lummoshop.com') for i in links2]\n",
    "links = links1+links2\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bing lummoshop.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups3 = []\n",
    "url = 'https://www.bing.com/search?q=site%3Alummoshop.com'\n",
    "# options = Options()\n",
    "# options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups3.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        next_links = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#b_results > li.b_pag > nav > ul > li:last-child\")))\n",
    "        next_links.click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups3.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 246\n"
     ]
    }
   ],
   "source": [
    "links3=[]\n",
    "for soup in soups3:\n",
    "    links3.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://lummoshop.com/'))])\n",
    "links3=sum(links3,[])\n",
    "links3=[re.findall('https://lummoshop.com/[a-zA-Z0-9]+',link)[0] for link in links3]\n",
    "print(f'Total URL found: {len((set(links3)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bing tokko.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups4 = []\n",
    "url = 'https://www.bing.com/search?q=site%3Atokko.io'\n",
    "# options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups4.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        next_links = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"#b_results > li.b_pag > nav > ul > li:last-child\")))\n",
    "        next_links.click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups4.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 23\n"
     ]
    }
   ],
   "source": [
    "links4=[]\n",
    "for soup in soups4:\n",
    "    links4.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://tokko.io/'))])\n",
    "links4=sum(links4,[])\n",
    "links4=[re.findall('https://tokko.io/[a-zA-Z0-9]+',link)[0] for link in links4]\n",
    "links4=[i.replace('tokko.io','lummoshop.com') for i in links4]\n",
    "print(f'Total URL found: {len((set(links4)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "596\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3+links4\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Yahoo lummoshop.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups5 = []\n",
    "url = 'https://www.search.yahoo.com/search?q=site%3Alummoshop.com'\n",
    "# options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups5.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        next_links = driver.find_elements(by=By.LINK_TEXT, value='Next')\n",
    "        next_links[0].click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups5.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 349\n"
     ]
    }
   ],
   "source": [
    "links5=[]\n",
    "for soup in soups5:\n",
    "    links5.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://lummoshop.com/'))])\n",
    "links5=sum(links5,[])\n",
    "links5=[re.findall('https://lummoshop.com/[a-zA-Z0-9]+',link)[0] for link in links5]\n",
    "print(f'Total URL found: {len((set(links5)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3+links4+links5\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo tokko.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups6 = []\n",
    "url = 'https://www.search.yahoo.com/search?q=site%3Atokko.io'\n",
    "# options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups6.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        next_links = driver.find_elements(by=By.LINK_TEXT, value='Next')\n",
    "        next_links[0].click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups6.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 37\n"
     ]
    }
   ],
   "source": [
    "links6=[]\n",
    "for soup in soups6:\n",
    "    links6.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://tokko.io/'))])\n",
    "links6=sum(links6,[])\n",
    "links6=[re.findall('https://tokko.io/[a-zA-Z0-9]+',link)[0] for link in links6]\n",
    "links6=[i.replace('tokko.io','lummoshop.com') for i in links6]\n",
    "print(f'Total URL found: {len((set(links6)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3+links4+links5+links6\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duckduckgo lummoshop.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/hw78fp696kq214rqw84h1tc80000gn/T/ipykernel_12021/247368710.py:12: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  driver.find_element_by_css_selector(\".is-link-style-exp .result--more__btn\").click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more show more button\n"
     ]
    }
   ],
   "source": [
    "soups7 = []\n",
    "url = 'https://duckduckgo.com/?q=site%3Alummoshop.com&t=h_&ia=web'\n",
    "# options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups7.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\".is-link-style-exp .result--more__btn\").click()\n",
    "        # show_more = driver.find_element(by=By.XPATH, value='//*[@id=\"rld-4\"]')\n",
    "        # driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print('No more show more button')\n",
    "        break\n",
    "    soups7.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 307\n"
     ]
    }
   ],
   "source": [
    "links7=[]\n",
    "for soup in soups7:\n",
    "    links7.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://lummoshop.com/'))])\n",
    "links7=sum(links7,[])\n",
    "links7=[re.findall('https://lummoshop.com/[a-zA-Z0-9]+',link)[0] for link in links7]\n",
    "print(f'Total URL found: {len((set(links7)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "987\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3+links4+links5+links6+links7\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duckduckgo tokko.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/hw78fp696kq214rqw84h1tc80000gn/T/ipykernel_12021/738873491.py:12: DeprecationWarning: find_element_by_css_selector is deprecated. Please use find_element(by=By.CSS_SELECTOR, value=css_selector) instead\n",
      "  driver.find_element_by_css_selector(\".is-link-style-exp .result--more__btn\").click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more show more button\n"
     ]
    }
   ],
   "source": [
    "soups8 = []\n",
    "url = 'https://duckduckgo.com/?q=site%3Atokko.io&t=h_&ia=web'\n",
    "# options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups8.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\".is-link-style-exp .result--more__btn\").click()\n",
    "        # show_more = driver.find_element(by=By.XPATH, value='//*[@id=\"rld-4\"]')\n",
    "        # driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        print('No more show more button')\n",
    "        break\n",
    "    soups8.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 34\n"
     ]
    }
   ],
   "source": [
    "links8=[]\n",
    "for soup in soups8:\n",
    "    links8.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://tokko.io/'))])\n",
    "links8=sum(links8,[])\n",
    "links8=[re.findall('https://tokko.io/[a-zA-Z0-9]+',link)[0] for link in links8]\n",
    "links8=[i.replace('tokko.io','lummoshop.com') for i in links8]\n",
    "print(f'Total URL found: {len((set(links8)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3+links4+links5+links6+links7+links8\n",
    "links = (set(links))\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swisscows lummoshop.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more next button\n"
     ]
    }
   ],
   "source": [
    "soups9 = []\n",
    "url = 'https://swisscows.com/web?query=site%3Alummoshop.com'\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "# options.add_argument(\"user-data-dir=selenium\")\n",
    "driver = webdriver.Chrome(options=options)    # creating chrome instance\n",
    "driver.get(url)\n",
    "soups9.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)\n",
    "while True:\n",
    "    try:\n",
    "        # next_links = driver.find_element_by_css_selector(\".web-pagination li.named.next\").click()\n",
    "        # print(next_links)\n",
    "        # next_links.click()\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(0.5)\n",
    "        next_links = driver.find_elements(by=By.LINK_TEXT, value='Next')\n",
    "        next_links[0].click()\n",
    "        time.sleep(3)\n",
    "    except:\n",
    "        print('No more next button')\n",
    "        break\n",
    "    soups9.append(BeautifulSoup(driver.page_source,'html.parser'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL found: 195\n"
     ]
    }
   ],
   "source": [
    "links9=[]\n",
    "for soup in soups9:\n",
    "    links9.append([i.get('href') for i in soup.findAll('a',href=re.compile('^https://lummoshop.com/'))])\n",
    "links9=sum(links9,[])\n",
    "links9=[re.findall('https://lummoshop.com/[a-zA-Z0-9]+',link)[0] for link in links9]\n",
    "print(f'Total URL found: {len((set(links9)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL=: 1042\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "links = links1+links2+links3+links4+links5+links6+links7+links8+links9\n",
    "links = (set(links))\n",
    "print(f'Total URL=: {len(links)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver    # to  control the chrome browser\n",
    "import time\n",
    "from bs4 import BeautifulSoup     # to parse the page source\n",
    "import pandas as pd                # to create csv file of scraped user details\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5z/hw78fp696kq214rqw84h1tc80000gn/T/ipykernel_12021/3385179379.py:7: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chrome_options=options)    # creating chrome instance\n"
     ]
    }
   ],
   "source": [
    "htmls=[]\n",
    "i=1\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=/Users/raihanafiandi/Library/Application Support/Google/Chrome/Default\")\n",
    "driver = webdriver.Chrome(chrome_options=options)    # creating chrome instance\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(0.5)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    time.sleep(2)\n",
    "    ss = driver.page_source\n",
    "    htmls.append(BeautifulSoup(ss,'html.parser'))\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(htmls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "product={}\n",
    "for i in range(1,101):\n",
    "    product[f'product_{i}']=[]\n",
    "    product[f'product_{i}_price']=[]\n",
    "    product[f'product_{i}_terjual']=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alamat=[]\n",
    "ulasan=[]\n",
    "banyak_ulasan=[]\n",
    "kepuasan=[]\n",
    "terjual=[]\n",
    "whatsapp=[]\n",
    "# product_name=[]\n",
    "# product_price=[]\n",
    "# product_sold=[]\n",
    "# product_1=[]\n",
    "# product_2=[]\n",
    "# product_3=[]\n",
    "# product_4=[]\n",
    "# product_5=[]\n",
    "\n",
    "# product_1_price=[]\n",
    "# product_2_price=[]\n",
    "# product_3_price=[]\n",
    "# product_4_price=[]\n",
    "# product_5_price=[]\n",
    "\n",
    "# product_1_terjual=[]\n",
    "# product_2_terjual=[]  \n",
    "# product_3_terjual=[]\n",
    "# product_4_terjual=[]\n",
    "# product_5_terjual=[]\n",
    "\n",
    "\n",
    "\n",
    "for idx,html in enumerate(htmls):\n",
    "    try:\n",
    "        alamat.append(html.find('div',id = 'header-address').text) # alamat\n",
    "    except AttributeError:\n",
    "        alamat.append('Data tidak tersedia')\n",
    "    try:\n",
    "        ulasan.append(html.find('div',id = 'rating').text) # ulasan\n",
    "    except AttributeError:\n",
    "        ulasan.append('ulasan tidak tersedia')\n",
    "    try:\n",
    "        banyak_ulasan.append(re.findall('[0-9]{1,2}|[0-9]{2,3}|[0-9]{3,4}',html.find('div',id = 'no-of-users-rated').text)[0]) # banyak ulasan\n",
    "    except AttributeError:\n",
    "        banyak_ulasan.append('banyak ulasan tidak tersedia')\n",
    "    try:\n",
    "        kepuasan.append(html.find('div',id = 'percentage-satisfaction').text) #kepuasan\n",
    "    except AttributeError:\n",
    "        kepuasan.append('data kepuasan tidak tersedia')\n",
    "    try:\n",
    "        terjual.append(html.find('div',id = 'item-sold-count').text) #barang terjual \n",
    "    except AttributeError:\n",
    "        terjual.append('data barang terjual tidak tersedia')\n",
    "    try:\n",
    "        whatsapp.append(re.findall('\\+?([ -]?\\d+)+|\\(\\d+\\)([ -]\\d+)',html.find('a',id = 'contact_via_whatsApp_element').get('href'))[0][0]) #whatsapp\n",
    "    except AttributeError:\n",
    "        whatsapp.append('No whatsapp')\n",
    "\n",
    "    for i in range(1,101):\n",
    "        try:\n",
    "            product[f'product_{i}'].append(html.find('div',id=f'product-card-list-{i}-name').text)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                product[f'product_{i}'].append(html.find('div',id=f'product-card-grid-{i}-name').text)\n",
    "            except AttributeError:\n",
    "                product[f'product_{i}'].append(f'Produk toko tidak mencapai {i}')\n",
    "\n",
    "        try:\n",
    "            product[f'product_{i}_price'].append(html.find(f'div',id='product-card-list-{i}-rate').find('div',{\"class\":\"Rate__Price-sc-14ldmny-1 iTwBHK list-price\"}).text)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                product[f'product_{i}_price'].append(html.find(f'div',id='product-card-grid-{i}-rate').find('div',{\"class\":\"Rate__Price-sc-14ldmny-1 iTwBHK grid-price\"}).text)\n",
    "            except AttributeError:\n",
    "                product[f'product_{i}_price'].append(f'Produk toko tidak mencapai {i}')\n",
    "            \n",
    "        try:\n",
    "            product[f'product_{i}_terjual'].append(html.find('div',id=f'product-card-list-{i}-item-count').text)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                product[f'product_{i}_terjual'].append(html.find('div',id=f'product-card-grid-{i}-item-count').text)\n",
    "            except AttributeError:\n",
    "                product[f'product_{i}_terjual'].append(f'Produk toko tidak mencapai {i}')\n",
    "\n",
    "    #     except AttributeError:\n",
    "    #         try:\n",
    "    #             product_name.append(html.find('div',id=f'product-card-grid-{i}-name').text)\n",
    "    #             product_price.append(html.find('div',id=f'product-card-grid-{i}-rate').text)\n",
    "    #             product_sold.append(html.find('div',id=f'product-card-grid-{i}-item-count').text)\n",
    "    #         except AttributeError:\n",
    "    #             product_sold.append('Data tidak tersedia')\n",
    "                \n",
    "    # for i,j in zip(range(1,100),range(len(product_name))):\n",
    "    #     try:\n",
    "    #         product[f'product_{i}'] = product_name[j]\n",
    "    #         product[f'product_{i}_price'] = product_price[j]\n",
    "    #         product[f'product_{i}_terjual']=product_sold[j]\n",
    "    #     except IndexError:\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/core/internals/construction.py:540: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(htmls).to_csv('htmls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "alamat=[]\n",
    "ulasan=[]\n",
    "banyak_ulasan=[]\n",
    "kepuasan=[]\n",
    "terjual=[]\n",
    "whatsapp=[]\n",
    "\n",
    "for idx,html in enumerate(htmls):\n",
    "    try:\n",
    "        alamat.append(html.find('div',id = 'header-address').text) # alamat\n",
    "    except AttributeError:\n",
    "        alamat.append('Data tidak tersedia')\n",
    "    try:\n",
    "        ulasan.append(html.find('div',id = 'rating').text) # ulasan\n",
    "    except AttributeError:\n",
    "        ulasan.append('ulasan tidak tersedia')\n",
    "    try:\n",
    "        banyak_ulasan.append(re.findall('[0-9]{1,2}|[0-9]{2,3}|[0-9]{3,4}',html.find('div',id = 'no-of-users-rated').text)[0]) # banyak ulasan\n",
    "    except AttributeError:\n",
    "        banyak_ulasan.append('banyak ulasan tidak tersedia')\n",
    "    try:\n",
    "        kepuasan.append(html.find('div',id = 'percentage-satisfaction').text) #kepuasan\n",
    "    except AttributeError:\n",
    "        kepuasan.append('data kepuasan tidak tersedia')\n",
    "    try:\n",
    "        terjual.append(html.find('div',id = 'item-sold-count').text) #barang terjual \n",
    "    except AttributeError:\n",
    "        terjual.append('data barang terjual tidak tersedia')\n",
    "    try:\n",
    "        whatsapp.append(re.findall('\\+?([ -]?\\d+)+|\\(\\d+\\)([ -]\\d+)',html.find('a',id = 'contact_via_whatsApp_element').get('href'))[0][0]) #whatsapp\n",
    "    except AttributeError:\n",
    "        whatsapp.append('No whatsapp')\n",
    "\n",
    "    for i in range(1,101):\n",
    "        try:\n",
    "            product[f'product_{i}'].append(html.find('div',id=f'product-card-list-{i}-name').text)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                product[f'product_{i}'].append(html.find('div',id=f'product-card-grid-{i}-name').text)\n",
    "            except AttributeError:\n",
    "                product[f'product_{i}'].append(f'Produk toko tidak mencapai {i}')\n",
    "\n",
    "        try:\n",
    "            product[f'product_{i}_price'].append(html.find('div',id=f'product-card-list-{i}-rate').text)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                product[f'product_{i}_price'].append(html.find('div',id=f'product-card-grid-{i}-rate').text)\n",
    "            except AttributeError:\n",
    "                product[f'product_{i}_price'].append(f'Produk toko tidak mencapai {i}')\n",
    "            \n",
    "        try:\n",
    "            product[f'product_{i}_terjual'].append(html.find('div',id=f'product-card-list-{i}-item-count').text)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                product[f'product_{i}_terjual'].append(html.find('div',id=f'product-card-grid-{i}-item-count').text)\n",
    "            except AttributeError:\n",
    "                product[f'product_{i}_terjual'].append(f'Produk toko tidak mencapai {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={\n",
    "    \"URL\":links,\n",
    "    \"Alamat\":alamat,\n",
    "    \"Whatsapp\":whatsapp,\n",
    "    \"Ulasan\":ulasan,\n",
    "    \"No ulasan\":banyak_ulasan,\n",
    "    \"Kepuasan\":kepuasan,\n",
    "    \"Barang terjual\":terjual\n",
    "    }\n",
    "dict.update(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product name length: 1042\n",
      "Link length: 1042\n"
     ]
    }
   ],
   "source": [
    "print(f\"Product name length: {len(product['product_100'])}\")\n",
    "print(f'Link length: {len(links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dict).to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(links).to_csv('links.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
